{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import GooglePalm\n",
    "api_key = 'AIzaSyBXtQlO-v2h7M5Or9G7cu1bFoYY6xRPY9c' \n",
    "\n",
    "# llm = GooglePalm(google_api_key=api_key, temperature=0.7)\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crispy pastry, golden brown,\n",
      "Filled with spices, a savory crown.\n",
      "Samosa, my heart's delight,\n",
      "A taste of heaven, day or night.\n"
     ]
    }
   ],
   "source": [
    "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a prompt template for the chatbot\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"The following is a conversation with a chatbot. The chatbot is helpful, creative, and very friendly. \\n\\nUser: {user_input}\\nChatbot:\"\n",
    ")\n",
    "\n",
    "# Create an LLMChain instance with the prompt template and the LLM\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': 'Who found google ?', 'text': 'Larry Page and Sergey Brin'}\n"
     ]
    }
   ],
   "source": [
    "res =  llm_chain.invoke(\"Who found google ?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': 'What are the nationalities of them ?', 'text': \"I'm sorry, but you haven't provided me with any context or information about the individuals in question, so I cannot determine their nationalities.\"}\n"
     ]
    }
   ],
   "source": [
    "res =  llm_chain.invoke(\"What are the nationalities of them ?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:125: UserWarning: Unknown PDF Filter!\n",
      "  warnings.warn(\"Unknown PDF Filter!\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory\n",
    "pdf_directory = './server/food_data/pdf'\n",
    "\n",
    "# Get all file names in the directory\n",
    "pdf_file_names = os.listdir(pdf_directory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "pdf_loader = PyPDFDirectoryLoader(pdf_directory,extract_images=True)\n",
    "\n",
    "pdf_docs = pdf_loader.load_and_split()\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200\n",
    "        )  \n",
    "\n",
    "\n",
    "\n",
    "final_pdf_documents = (\n",
    "            text_splitter.split_documents(pdf_docs)\n",
    "        )  \n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",google_api_key=api_key\n",
    "        )\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_db = FAISS.from_documents(\n",
    "            pdf_docs, google_embeddings\n",
    "        )\n",
    "\n",
    "retriever = vector_db.as_retriever(score_threshold = 0.7)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5,memory_key=\"chat_history\",  return_messages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    " You are the specialized AI assistant for a food datas. Give most accurate answer to the following questions, Sometimes given context may be helpful And somtime you may need to use your Knowledge to answer the question. You can get sense from given chat history also\"\n",
    "CONTEXT: {context}\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            memory = memory,\n",
    "                            return_source_documents=False,\n",
    "                            chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweet potato fries, Mashed sweet potatoes, Baked sweet potatoes, Sweet potato casserole, Sweet potato soup, Sweet potato hash, Sweet potato pie\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"What We Can Prepare with Sweet Potatoes ?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='What We Can Prepare with Sweet Potatoes ?'), AIMessage(content='Sweet potato fries, Mashed sweet potatoes, Baked sweet potatoes, Sweet potato casserole, Sweet potato soup, Sweet potato hash, Sweet potato pie')]), memory_key='chat_history')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LLM chain chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an LLMChain instance with the prompt template and the LLM\n",
    "llm_chain = LLMChain(prompt=PROMPT, llm=llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Sweet potato fries\n",
      "• Mashed sweet potatoes\n",
      "• Baked sweet potatoes\n",
      "• Sweet potato casserole\n",
      "• Sweet potato soup\n",
      "• Sweet potato hash\n",
      "• Sweet potato pie\n"
     ]
    }
   ],
   "source": [
    "question = \"What We Can Prepare with Sweet Potatoes ?\"\n",
    "context =  retriever.get_relevant_documents(\"What We Can Prepare with Sweet Potatoes\")\n",
    "res = llm_chain.invoke({'context': context, 'question': question})\n",
    "print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    context =  retriever.get_relevant_documents(question)\n",
    "    res = llm_chain.invoke({'context': context, 'question': question})\n",
    "    return res['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Sweet potato fries\n",
      "• Mashed sweet potatoes\n",
      "• Baked sweet potatoes\n",
      "• Sweet potato casserole\n",
      "• Sweet potato soup\n",
      "• Sweet potato hash\n",
      "• Sweet potato pie\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"What We Can Prepare with Sweet Potatoes ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain instructions on how to prepare each dish, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"How can we prepare each ?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Paneer:**\n",
      "\n",
      "- Fresh milk: 400 ml\n",
      "- Lime Juice: 1 lime\n",
      "\n",
      "**Meatballs:**\n",
      "\n",
      "- Chicken (skin and boneless): 100 g\n",
      "- Garlic paste: 5 g\n",
      "- Chickpea flour: 5 g\n",
      "- Soya flour: 5 g\n",
      "- Basil leaves powder: 0.5 g\n",
      "- Thyme powder: 0.5 g\n",
      "\n",
      "**Masala curry mixture:**\n",
      "\n",
      "- Tomato: 200 g\n",
      "- Water: 100 g\n",
      "- Cashew: 20 g\n",
      "- Ginger paste: 5 g\n",
      "- Green chili: 5\n",
      "- Coriander leaves: 3\n",
      "- Cinnamon: 1\n",
      "- Red chili: 1\n",
      "- Cardamon: 0.5\n",
      "- Bay leaves: 0.5\n",
      "- Salt: 1 g\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"What are the ingredients of  Paneer & Meatball Masala Curry and give how much amount of each ingrdient used?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Boil the fresh milk to 80oC and add lime juice and allow the mixture to curdle\n",
      "2. Then separate the whey by straining through a clean white cloth\n",
      "3. Allow the mixture to set by pressing it with a heavy weight (around 2kg)\n",
      "4. Keep in the freezer for 15 mins\n",
      "5. Cut it into cubes\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"Give  Method of preparation of Paneer ?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not mention anything about the capital of India, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"Capital of India\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not mention anything about the capital of India, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(\"Capital of India ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Custom Memory Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom memory stack (empty at first)\n",
    "custom_memory_stack = [\"Assistant : Hi ! I am specialized AI assistant for a food datas \"]\n",
    "\n",
    "\n",
    "# Stack size limit (if you want to limit the chat history)\n",
    "STACK_SIZE_LIMIT = 20\n",
    "\n",
    "# Function to update the memory stack and maintain stack size\n",
    "def update_memory_stack(question, response, memory_stack, limit=STACK_SIZE_LIMIT):\n",
    "    # Add the new question and response to the stack\n",
    "    memory_stack.append(f\"User: {question}\")\n",
    "    memory_stack.append(f\"Assistant: {response}\")\n",
    "    \n",
    "    # Ensure the stack doesn't exceed the limit\n",
    "    if len(memory_stack) > 2 * limit:  # each interaction is 2 entries (question & response)\n",
    "        memory_stack = memory_stack[-2 * limit:]  # Keep only the last `limit` interactions\n",
    "    \n",
    "    return memory_stack\n",
    "\n",
    "def clear_memory_stack(memory_stack):\n",
    "    memory_stack = [\"Assistant : Hi ! I am specialized AI assistant for a food datas \"]\n",
    "    return memory_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
