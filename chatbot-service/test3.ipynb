{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import GooglePalm\n",
    "api_key = 'AIzaSyBXtQlO-v2h7M5Or9G7cu1bFoYY6xRPY9c' \n",
    "\n",
    "# llm = GooglePalm(google_api_key=api_key, temperature=0.7)\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crispy crust, a savory treat,\n",
      "Samosa, my heart's ardent beat.\n",
      "Potato and peas, a perfect blend,\n",
      "A taste of heaven, unending end.\n"
     ]
    }
   ],
   "source": [
    "poem = llm(\"Write a 4 line poem of my love for samosa\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a prompt template for the chatbot\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"The following is a conversation with a chatbot. The chatbot is helpful, creative, and very friendly. \\n\\nUser: {user_input}\\nChatbot:\"\n",
    ")\n",
    "\n",
    "# Create an LLMChain instance with the prompt template and the LLM\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': 'Who found google ?', 'text': 'Larry Page and Sergey Brin'}\n"
     ]
    }
   ],
   "source": [
    "res =  llm_chain.invoke(\"Who found google ?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': 'What are the nationalities of them ?', 'text': \"I'm not sure what you mean. Can you please rephrase your question?\"}\n"
     ]
    }
   ],
   "source": [
    "res =  llm_chain.invoke(\"What are the nationalities of them ?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory\n",
    "pdf_directory = './server/food_data/pdf'\n",
    "\n",
    "# Get all file names in the directory\n",
    "pdf_file_names = os.listdir(pdf_directory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "pdf_loader = PyPDFDirectoryLoader(pdf_directory,extract_images=True)\n",
    "\n",
    "pdf_docs = pdf_loader.load_and_split()\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=200\n",
    "        )  \n",
    "\n",
    "\n",
    "\n",
    "final_pdf_documents = (\n",
    "            text_splitter.split_documents(pdf_docs)\n",
    "        )  \n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",google_api_key=api_key\n",
    "        )\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_db = FAISS.from_documents(\n",
    "            pdf_docs, google_embeddings\n",
    "        )\n",
    "\n",
    "retriever = vector_db.as_retriever(score_threshold = 0.7)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5,memory_key=\"chat_history\",  return_messages=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    " You are the specialized AI assistant for a food datas. Give most accurate answer to the following questions, Sometimes given context may be helpful And somtime you may need to use your Knowledge to answer the question. You can get sense from given chat history also\"\n",
    "CONTEXT: {context}\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\",\"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            memory = memory,\n",
    "                            return_source_documents=False,\n",
    "                            chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Sweet potato fries\n",
      "• Mashed sweet potatoes\n",
      "• Baked sweet potatoes\n",
      "• Sweet potato casserole\n",
      "• Sweet potato soup\n",
      "• Sweet potato hash\n",
      "• Sweet potato pie\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"What We Can Prepare with Sweet Potatoes ?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guacamole is a Mexican dish made from mashed avocado, with various seasonings, most commonly including salt, lime juice, and cilantro. It is often served as a dip, with tortilla chips, or as a topping for tacos or burritos.\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"what is Guacamole ?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ingredients for the coconut curry are:\n",
      "\n",
      "1 can (14 oz) coconut milk\n",
      "1 cup vegetable broth\n",
      "1 pound vegetables (such as bell peppers, carrots, and broccoli), chopped\n",
      "1 block (14 oz) tofu or chicken, cubed (optional)\n",
      "Salt and pepper to taste\n",
      "Fresh cilantro for garnish\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"what are the ingredients that want to it?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='What We Can Prepare with Sweet Potatoes ?'), AIMessage(content='• Sweet potato fries\\n• Mashed sweet potatoes\\n• Baked sweet potatoes\\n• Sweet potato casserole\\n• Sweet potato soup\\n• Sweet potato hash\\n• Sweet potato pie')]), memory_key='chat_history')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LLM chain chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an LLMChain instance with the prompt template and the LLM\n",
    "llm_chain = LLMChain(prompt=PROMPT, llm=llm )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Sweet potato fries\n",
      "• Mashed sweet potatoes\n",
      "• Baked sweet potatoes\n",
      "• Sweet potato casserole\n",
      "• Sweet potato soup\n",
      "• Sweet potato hash\n",
      "• Sweet potato pie\n"
     ]
    }
   ],
   "source": [
    "question = \"What We Can Prepare with Sweet Potatoes ?\"\n",
    "context =  retriever.get_relevant_documents(\"What We Can Prepare with Sweet Potatoes\")\n",
    "res = llm_chain.invoke({'context': context, 'question': question})\n",
    "print(res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = \"\"\"\n",
    " \"You are a specialized AI assistant for food data. Provide the most accurate and helpful answer to the following questions. You have access to both the current context and the chat history (memory), which may contain relevant information from previous interactions. Use the context, your knowledge, and the chat history to form the best answer.\"\n",
    "CONTEXT: {context}\n",
    "QUESTION: {question}\n",
    "CHAT_HISTORY: {chat_history}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\",\"question\" ,\"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.memory == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not contain the ingredients or instructions for Paneer & Meatball Masala Curry. So I cannot extract the requested data from the provided context.\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"What are the ingredients of  Paneer & Meatball Masala Curry and give how much amount of each ingrdient used?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not mention anything about Paneer, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"Give  Method of preparation of Paneer ?\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not mention anything about the capital of India, so I cannot answer this question from the provided context.\n"
     ]
    }
   ],
   "source": [
    "res =  chain.invoke(\"Capital of India\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Custom Memory Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom memory stack (empty at first)\n",
    "custom_memory_stack = [\"Assistant : Hi ! I am specialized AI assistant for a food datas \"]\n",
    "\n",
    "\n",
    "# Stack size limit (if you want to limit the chat history)\n",
    "STACK_SIZE_LIMIT = 3\n",
    "\n",
    "\n",
    "# Function to format memory into a string (join the stack)\n",
    "def format_memory(memory_stack):\n",
    "    return \"\\n\".join(memory_stack)\n",
    "\n",
    "# Function to update the memory stack and maintain stack size\n",
    "def update_memory_stack(question, response, memory_stack, limit=STACK_SIZE_LIMIT):\n",
    "    # Add the new question and response to the stack\n",
    "    memory_stack.append(f\"User: {question}\")\n",
    "    memory_stack.append(f\"Assistant: {response}\")\n",
    "    \n",
    "    # Ensure the stack doesn't exceed the limit\n",
    "    if len(memory_stack) > 2 * limit:  # each interaction is 2 entries (question & response)\n",
    "        memory_stack = memory_stack[-2 * limit:]  # Keep only the last `limit` interactions\n",
    "    \n",
    "    return memory_stack\n",
    "\n",
    "def clear_memory_stack(memory_stack):\n",
    "    memory_stack = [\"Assistant : Hi ! I am specialized AI assistant for a food datas \"]\n",
    "    return memory_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question , custom_memory_stack):\n",
    "     # Pass the query and the formatted memory to the chain\n",
    "    formatted_memory = format_memory(custom_memory_stack)\n",
    "    print(f\"Memory: {formatted_memory}\")\n",
    "    context =  retriever.get_relevant_documents(question)\n",
    "    res = llm_chain.invoke({'context': context, 'question': question , 'chat_history':formatted_memory})\n",
    "    print(res['text'])\n",
    "    # Update the memory stack with the new question and response\n",
    "    custom_memory_stack = update_memory_stack(question, res['text'], custom_memory_stack)\n",
    "    print(custom_memory_stack)\n",
    "    return res['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in my chatbot implementation i used langchain LLM chain with my custom memory, \n",
    "# but  memory does not work well\n",
    "\n",
    "# this is my implementation\n",
    "\n",
    "# # Create an LLMChain instance with the prompt template and the LLM\n",
    "# llm_chain = LLMChain(prompt=PROMPT, llm=llm )\n",
    "\n",
    "# prompt_template2 = \"\"\"\n",
    "#  \"You are a specialized AI assistant for food data. Provide the most accurate and helpful answer to the following questions. You have access to both the current context and the chat history (memory), which may contain relevant information from previous interactions. Use the context, your knowledge, and the chat history to form the best answer.\"\n",
    "# CONTEXT: {context}\n",
    "# QUESTION: {question}\n",
    "# CHAT_HISTORY: {chat_history}\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# PROMPT = PromptTemplate(\n",
    "#     template=prompt_template, input_variables=[\"context\",\"question\" ,\"chat_history\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# ----- Custom Memory ----\n",
    "# # Initialize custom memory stack (empty at first)\n",
    "# custom_memory_stack = [\"Assistant : Hi ! I am specialized AI assistant for a food datas \"]\n",
    "\n",
    "\n",
    "# # Stack size limit (if you want to limit the chat history)\n",
    "# STACK_SIZE_LIMIT = 3\n",
    "\n",
    "\n",
    "# # Function to format memory into a string (join the stack)\n",
    "# def format_memory(memory_stack):\n",
    "#     return \"\\n\".join(memory_stack)\n",
    "\n",
    "# # Function to update the memory stack and maintain stack size\n",
    "# def update_memory_stack(question, response, memory_stack, limit=STACK_SIZE_LIMIT):\n",
    "#     # Add the new question and response to the stack\n",
    "#     memory_stack.append(f\"User: {question}\")\n",
    "#     memory_stack.append(f\"Assistant: {response}\")\n",
    "    \n",
    "#     # Ensure the stack doesn't exceed the limit\n",
    "#     if len(memory_stack) > 2 * limit:  # each interaction is 2 entries (question & response)\n",
    "#         memory_stack = memory_stack[-2 * limit:]  # Keep only the last `limit` interactions\n",
    "    \n",
    "#     return memory_stack\n",
    "\n",
    "# def clear_memory_stack(memory_stack):\n",
    "#     memory_stack = [\"Assistant : Hi ! I am specialized AI assistant for a food datas \"]\n",
    "#     return memory_stack\n",
    "\n",
    "\n",
    "# def get_answer(question , custom_memory_stack):\n",
    "#      # Pass the query and the formatted memory to the chain\n",
    "#     formatted_memory = format_memory(custom_memory_stack)\n",
    "#     print(f\"Memory: {formatted_memory}\")\n",
    "#     context =  retriever.get_relevant_documents(question)\n",
    "#     res = llm_chain.invoke({'context': context, 'question': question , 'chat_history':formatted_memory})\n",
    "#     print(res['text'])\n",
    "#     # Update the memory stack with the new question and response\n",
    "#     custom_memory_stack = update_memory_stack(question, res['text'], custom_memory_stack)\n",
    "#     print(custom_memory_stack)\n",
    "#     return res['text']\n",
    "\n",
    "\n",
    "# now i get responses from it\n",
    "\n",
    "# get_answer(\"what is Guacamole ?\",custom_memory_stack=custom_memory_stack)\n",
    "# this gives correct answer\n",
    "\n",
    "# but \n",
    "\n",
    "# get_answer(\"what are the ingredients that want to it?\",custom_memory_stack=custom_memory_stack)\n",
    "\n",
    "# in this does not correctly identify what is means by 'it'\n",
    "\n",
    "# how can i solve this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: Assistant : Hi ! I am specialized AI assistant for a food datas \n",
      "Guacamole is a Mexican dish made from mashed avocado, lime juice, onions, cilantro, and salt. It is typically served as a dip with tortilla chips, but can also be used as a sandwich spread or topping for tacos and burritos.\n",
      "['Assistant : Hi ! I am specialized AI assistant for a food datas ', 'User: what is Guacamole ?', 'Assistant: Guacamole is a Mexican dish made from mashed avocado, lime juice, onions, cilantro, and salt. It is typically served as a dip with tortilla chips, but can also be used as a sandwich spread or topping for tacos and burritos.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Guacamole is a Mexican dish made from mashed avocado, lime juice, onions, cilantro, and salt. It is typically served as a dip with tortilla chips, but can also be used as a sandwich spread or topping for tacos and burritos.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"what is Guacamole ?\",custom_memory_stack=custom_memory_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: Assistant : Hi ! I am specialized AI assistant for a food datas \n",
      "User: what is Guacamole ?\n",
      "Assistant: Guacamole is a Mexican dish made from mashed avocado, lime juice, onions, cilantro, and salt. It is typically served as a dip with tortilla chips, but can also be used as a sandwich spread or topping for tacos and burritos.\n",
      "The ingredients needed to make coconut macaroons are:\n",
      "- 2 cups shredded coconut\n",
      "- 1/2 cup sweetened condensed milk\n",
      "- 1 teaspoon vanilla extract\n",
      "- 2 egg whites\n",
      "- Pinch of salt\n",
      "['Assistant : Hi ! I am specialized AI assistant for a food datas ', 'User: what is Guacamole ?', 'Assistant: Guacamole is a Mexican dish made from mashed avocado, lime juice, onions, cilantro, and salt. It is typically served as a dip with tortilla chips, but can also be used as a sandwich spread or topping for tacos and burritos.', 'User: what are the ingredients that want to it?', 'Assistant: The ingredients needed to make coconut macaroons are:\\n- 2 cups shredded coconut\\n- 1/2 cup sweetened condensed milk\\n- 1 teaspoon vanilla extract\\n- 2 egg whites\\n- Pinch of salt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The ingredients needed to make coconut macaroons are:\\n- 2 cups shredded coconut\\n- 1/2 cup sweetened condensed milk\\n- 1 teaspoon vanilla extract\\n- 2 egg whites\\n- Pinch of salt'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(\"what are the ingredients that want to it?\",custom_memory_stack=custom_memory_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with your provided memory\n",
    "raw_memory = \"\"\"\n",
    "Assistant : Hi ! I am specialized AI assistant for a food datas \n",
    "User: what is Guacamole ?\n",
    "Assistant: Guacamole is a Mexican dish made from mashed avocado, lime juice, onions, cilantro, and salt. It is typically served as a dip with tortilla chips, but can also be used as a sandwich spread or topping for tacos and burritos.\n",
    "The ingredients needed to make coconut macaroons are:\n",
    "- 2 cups shredded coconut\n",
    "- 1/2 cup sweetened condensed milk\n",
    "- 1 teaspoon vanilla extract\n",
    "- 2 egg whites\n",
    "- Pinch of salt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ingredients needed to make coconut macaroons are:\\n\\n- 2 cups shredded coconut\\n- 1/2 cup sweetened condensed milk\\n- 1 teaspoon vanilla extract\\n- 2 egg whites\\n- Pinch of salt'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(raw_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
